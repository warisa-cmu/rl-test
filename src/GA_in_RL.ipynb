{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6533ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/01 02:05:28 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: BaseGA(epoch=20, pop_size=10, pc=0.85, pm=0.1)\n",
      "2025/07/01 02:05:29 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 1, Current best: -38.0, Global best: -41.5, Runtime: 0.55542 seconds\n",
      "2025/07/01 02:05:30 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 2, Current best: -53.1, Global best: -53.1, Runtime: 0.68382 seconds\n",
      "2025/07/01 02:05:30 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 3, Current best: -53.1, Global best: -53.1, Runtime: 0.67910 seconds\n",
      "2025/07/01 02:05:31 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 4, Current best: -65.4, Global best: -65.4, Runtime: 0.72986 seconds\n",
      "2025/07/01 02:05:32 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 5, Current best: -65.4, Global best: -65.4, Runtime: 0.81023 seconds\n",
      "2025/07/01 02:05:33 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 6, Current best: -67.1, Global best: -67.1, Runtime: 0.86944 seconds\n",
      "2025/07/01 02:05:33 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 7, Current best: -59.2, Global best: -67.1, Runtime: 0.68085 seconds\n",
      "2025/07/01 02:05:34 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 8, Current best: -59.2, Global best: -67.1, Runtime: 0.78673 seconds\n",
      "2025/07/01 02:05:35 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 9, Current best: -59.2, Global best: -67.1, Runtime: 0.79450 seconds\n",
      "2025/07/01 02:05:36 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 10, Current best: -63.6, Global best: -67.1, Runtime: 0.81177 seconds\n",
      "2025/07/01 02:05:36 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 11, Current best: -59.2, Global best: -67.1, Runtime: 0.73098 seconds\n",
      "2025/07/01 02:05:37 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 12, Current best: -72.7, Global best: -72.7, Runtime: 0.78935 seconds\n",
      "2025/07/01 02:05:38 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 13, Current best: -59.2, Global best: -72.7, Runtime: 0.71484 seconds\n",
      "2025/07/01 02:05:39 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 14, Current best: -59.2, Global best: -72.7, Runtime: 0.62360 seconds\n",
      "2025/07/01 02:05:39 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 15, Current best: -61.9, Global best: -72.7, Runtime: 0.69984 seconds\n",
      "2025/07/01 02:05:40 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 16, Current best: -62.9, Global best: -72.7, Runtime: 0.72054 seconds\n",
      "2025/07/01 02:05:41 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 17, Current best: -62.9, Global best: -72.7, Runtime: 0.74024 seconds\n",
      "2025/07/01 02:05:41 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 18, Current best: -62.9, Global best: -72.7, Runtime: 0.67553 seconds\n",
      "2025/07/01 02:05:42 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 19, Current best: -61.9, Global best: -72.7, Runtime: 0.82954 seconds\n",
      "2025/07/01 02:05:43 PM, INFO, mealpy.evolutionary_based.GA.BaseGA: >>>Problem: P, Epoch: 20, Current best: -59.2, Global best: -72.7, Runtime: 0.64682 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "Learning Rate: 0.7996\n",
      "Discount Factor: 0.7772\n",
      "Epsilon: 0.2217\n",
      "Best Average Reward: 72.70\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from mealpy import FloatVar, GA\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetaheuristicQLearning:\n",
    "    def __init__(self, env_name=\"CartPole-v1\"):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "        # Discretize continuous state space for Q-table\n",
    "        self.discrete_bins = 20\n",
    "        self.state_bounds = list(zip(self.env.observation_space.low, \n",
    "                                   self.env.observation_space.high))\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to discrete state\"\"\"\n",
    "        discrete_state = []\n",
    "        for i, (low, high) in enumerate(self.state_bounds):\n",
    "            # Handle infinite bounds\n",
    "            if np.isinf(low):\n",
    "                low = -4.0\n",
    "            if np.isinf(high):\n",
    "                high = 4.0\n",
    "            \n",
    "            # Clip and discretize\n",
    "            state_val = np.clip(state[i], low, high)\n",
    "            discrete_val = int((state_val - low) / (high - low) * (self.discrete_bins - 1))\n",
    "            discrete_state.append(discrete_val)\n",
    "        return tuple(discrete_state)\n",
    "    \n",
    "    def q_learning_episode(self, learning_rate, discount_factor, epsilon):\n",
    "        \"\"\"Run a single Q-learning episode with given hyperparameters\"\"\"\n",
    "        # Initialize Q-table\n",
    "        q_table = np.zeros([self.discrete_bins] * self.state_size + [self.action_size])\n",
    "        \n",
    "        total_rewards = []\n",
    "        \n",
    "        for episode in range(100):  # Reduced episodes for optimization\n",
    "            state = self.env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            \n",
    "            discrete_state = self.discretize_state(state)\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Epsilon-greedy action selection\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(q_table[discrete_state])\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "                if isinstance(next_state, tuple):\n",
    "                    next_state = next_state[0]\n",
    "                \n",
    "                next_discrete_state = self.discretize_state(next_state)\n",
    "                \n",
    "                # Q-learning update\n",
    "                current_q = q_table[discrete_state + (action,)]\n",
    "                max_next_q = np.max(q_table[next_discrete_state])\n",
    "                new_q = current_q + learning_rate * (reward + discount_factor * max_next_q - current_q)\n",
    "                q_table[discrete_state + (action,)] = new_q\n",
    "                \n",
    "                discrete_state = next_discrete_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "        \n",
    "        return np.mean(total_rewards[-10:])  # Return average of last 10 episodes\n",
    "    \n",
    "    def objective_function(self, solution):\n",
    "        \"\"\"Objective function for metaheuristic optimization\"\"\"\n",
    "        learning_rate, discount_factor, epsilon = solution\n",
    "        \n",
    "        # Ensure parameters are in valid ranges\n",
    "        learning_rate = np.clip(learning_rate, 0.001, 1.0)\n",
    "        discount_factor = np.clip(discount_factor, 0.1, 0.99)\n",
    "        epsilon = np.clip(epsilon, 0.01, 1.0)\n",
    "        \n",
    "        # Run Q-learning with these parameters\n",
    "        avg_reward = self.q_learning_episode(learning_rate, discount_factor, epsilon)\n",
    "        \n",
    "        # Return negative reward (since we want to maximize reward but mealpy minimizes)\n",
    "        return -avg_reward\n",
    "    \n",
    "    def optimize_hyperparameters(self):\n",
    "        \"\"\"Use metaheuristic to optimize Q-learning hyperparameters\"\"\"\n",
    "        # Define problem bounds: [learning_rate, discount_factor, epsilon]\n",
    "        problem_dict = {\n",
    "            \"obj_func\": self.objective_function,\n",
    "            \"bounds\": FloatVar(lb=[0.001, 0.1, 0.01], ub=[1.0, 0.99, 1.0]),\n",
    "            \"minmax\": \"min\",\n",
    "        }\n",
    "        \n",
    "        # Use Genetic Algorithm to optimize\n",
    "        optimizer = GA.BaseGA(epoch=20, pop_size=10, pc=0.85, pm=0.1)\n",
    "        optimizer.solve(problem_dict)\n",
    "        \n",
    "        best_params = optimizer.g_best.solution\n",
    "        best_fitness = -optimizer.g_best.target.fitness\n",
    "        \n",
    "        print(f\"Best hyperparameters found:\")\n",
    "        print(f\"Learning Rate: {best_params[0]:.4f}\")\n",
    "        print(f\"Discount Factor: {best_params[1]:.4f}\")\n",
    "        print(f\"Epsilon: {best_params[2]:.4f}\")\n",
    "        print(f\"Best Average Reward: {best_fitness:.2f}\")\n",
    "        \n",
    "        return best_params, best_fitness\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the metaheuristic Q-learning optimizer\n",
    "    mql = MetaheuristicQLearning(\"CartPole-v1\")\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    best_params, best_reward = mql.optimize_hyperparameters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
