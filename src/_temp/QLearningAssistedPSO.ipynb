{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2927b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Sphere Function:\n",
      "Iteration 0: Best fitness = 22.744421\n",
      "Iteration 20: Best fitness = 17.883321\n",
      "Iteration 40: Best fitness = 15.446013\n",
      "Iteration 60: Best fitness = 15.446013\n",
      "Iteration 80: Best fitness = 14.456458\n",
      "Iteration 100: Best fitness = 14.456458\n",
      "Iteration 120: Best fitness = 14.456458\n",
      "Iteration 140: Best fitness = 11.804027\n",
      "Iteration 160: Best fitness = 10.579282\n",
      "Iteration 180: Best fitness = 10.579282\n",
      "\n",
      "Final Results:\n",
      "Best solution: [ 0.16943576 -1.87804194  0.96189846  1.01658864 -0.29824828  0.96479517\n",
      "  0.24910888 -0.20657894 -0.43963382  0.82075816]\n",
      "Best fitness: 7.505884817151466\n",
      "\n",
      "Learned Q-table has 3 states\n",
      "State early_high_poor:\n",
      "  uniform_crossover: 5902.8241\n",
      "  cauchy_mutation: 6435.3949\n",
      "  gaussian_mutation: 5967.6910\n",
      "  differential_mutation: 6464.7960\n",
      "  levy_flight: 5814.1263\n",
      "  opposition_based_learning: 6313.9384\n",
      "State middle_high_poor:\n",
      "  uniform_crossover: 11470.2163\n",
      "  differential_mutation: 11877.0376\n",
      "  opposition_based_learning: 11138.1915\n",
      "  gaussian_mutation: 11227.7608\n",
      "  levy_flight: 10919.1105\n",
      "  cauchy_mutation: 11592.4216\n",
      "State late_high_poor:\n",
      "  uniform_crossover: 18535.4005\n",
      "  differential_mutation: 26740.6403\n",
      "  cauchy_mutation: 18226.2300\n",
      "  opposition_based_learning: 16727.3166\n",
      "  levy_flight: 17918.6429\n",
      "  gaussian_mutation: 18867.7193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Monkeypatch\n",
    "import math\n",
    "np.math = math\n",
    "\n",
    "\n",
    "class QLearningAssistedPSO:\n",
    "    def __init__(self, dim, pop_size=30, max_iter=100):\n",
    "        self.dim = dim\n",
    "        self.pop_size = pop_size\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # PSO parameters\n",
    "        self.w = 0.9  # inertia weight\n",
    "        self.c1 = 2.0  # cognitive parameter\n",
    "        self.c2 = 2.0  # social parameter\n",
    "        \n",
    "        # Q-learning parameters\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.gamma = 0.9  # discount factor\n",
    "        self.epsilon = 0.1  # exploration rate\n",
    "        \n",
    "        # Q-table for state-action pairs\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Define local search operations\n",
    "        self.local_search_operations = [\n",
    "            'gaussian_mutation',\n",
    "            'levy_flight',\n",
    "            'cauchy_mutation',\n",
    "            'uniform_crossover',\n",
    "            'differential_mutation',\n",
    "            'opposition_based_learning'\n",
    "        ]\n",
    "        \n",
    "        # Initialize particles\n",
    "        self.particles = []\n",
    "        self.velocities = []\n",
    "        self.personal_best = []\n",
    "        self.personal_best_fitness = []\n",
    "        self.global_best = None\n",
    "        self.global_best_fitness = float('inf')\n",
    "        \n",
    "    def initialize_population(self, bounds):\n",
    "        \"\"\"Initialize PSO population\"\"\"\n",
    "        for i in range(self.pop_size):\n",
    "            particle = np.random.uniform(bounds[0], bounds[1], self.dim)\n",
    "            velocity = np.random.uniform(-1, 1, self.dim)\n",
    "            \n",
    "            self.particles.append(particle)\n",
    "            self.velocities.append(velocity)\n",
    "            self.personal_best.append(particle.copy())\n",
    "            self.personal_best_fitness.append(float('inf'))\n",
    "    \n",
    "    def get_state(self, iteration, diversity, improvement_rate):\n",
    "        \"\"\"Define state representation for Q-learning\"\"\"\n",
    "        # Discretize continuous values into states\n",
    "        iter_state = \"early\" if iteration < self.max_iter * 0.3 else \\\n",
    "                    \"middle\" if iteration < self.max_iter * 0.7 else \"late\"\n",
    "        \n",
    "        div_state = \"high\" if diversity > 0.5 else \\\n",
    "                   \"medium\" if diversity > 0.2 else \"low\"\n",
    "        \n",
    "        imp_state = \"good\" if improvement_rate > 0.1 else \\\n",
    "                   \"moderate\" if improvement_rate > 0.01 else \"poor\"\n",
    "        \n",
    "        return f\"{iter_state}_{div_state}_{imp_state}\"\n",
    "    \n",
    "    def calculate_diversity(self):\n",
    "        \"\"\"Calculate population diversity\"\"\"\n",
    "        if len(self.particles) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        center = np.mean(self.particles, axis=0)\n",
    "        distances = [np.linalg.norm(p - center) for p in self.particles]\n",
    "        return np.mean(distances) / np.sqrt(self.dim)\n",
    "    \n",
    "    def select_local_search_operation(self, state):\n",
    "        \"\"\"Q-learning based operation selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: random selection\n",
    "            return random.choice(self.local_search_operations)\n",
    "        else:\n",
    "            # Exploitation: select best action based on Q-values\n",
    "            q_values = self.q_table[state]\n",
    "            if not q_values:\n",
    "                return random.choice(self.local_search_operations)\n",
    "            return max(q_values.keys(), key=lambda k: q_values[k])\n",
    "    \n",
    "    def apply_local_search_operation(self, particle, operation):\n",
    "        \"\"\"Apply selected local search operation\"\"\"\n",
    "        new_particle = particle.copy()\n",
    "        \n",
    "        if operation == 'gaussian_mutation':\n",
    "            noise = np.random.normal(0, 0.1, self.dim)\n",
    "            new_particle += noise\n",
    "            \n",
    "        elif operation == 'levy_flight':\n",
    "            beta = 1.5\n",
    "            sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n",
    "                    (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n",
    "            u = np.random.normal(0, sigma, self.dim)\n",
    "            v = np.random.normal(0, 1, self.dim)\n",
    "            levy = u / (np.abs(v) ** (1 / beta))\n",
    "            new_particle += 0.01 * levy\n",
    "            \n",
    "        elif operation == 'cauchy_mutation':\n",
    "            cauchy_noise = np.random.standard_cauchy(self.dim) * 0.1\n",
    "            new_particle += cauchy_noise\n",
    "            \n",
    "        elif operation == 'uniform_crossover':\n",
    "            if len(self.particles) > 1:\n",
    "                partner = random.choice(self.particles)\n",
    "                mask = np.random.random(self.dim) < 0.5\n",
    "                new_particle = np.where(mask, particle, partner)\n",
    "                \n",
    "        elif operation == 'differential_mutation':\n",
    "            if len(self.particles) >= 3:\n",
    "                indices = random.sample(range(len(self.particles)), 3)\n",
    "                r1, r2, r3 = [self.particles[i] for i in indices]\n",
    "                F = 0.5\n",
    "                new_particle = r1 + F * (r2 - r3)\n",
    "                \n",
    "        elif operation == 'opposition_based_learning':\n",
    "            # Assuming bounds are [-10, 10] for simplicity\n",
    "            bounds_low, bounds_high = -10, 10\n",
    "            new_particle = bounds_low + bounds_high - particle\n",
    "        \n",
    "        return new_particle\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table using Q-learning update rule\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        # Find maximum Q-value for next state\n",
    "        next_q_values = self.q_table[next_state]\n",
    "        max_next_q = max(next_q_values.values()) if next_q_values else 0\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "    \n",
    "    def calculate_reward(self, old_fitness, new_fitness, diversity_change):\n",
    "        \"\"\"Calculate reward for Q-learning\"\"\"\n",
    "        fitness_improvement = old_fitness - new_fitness\n",
    "        \n",
    "        # Reward based on fitness improvement\n",
    "        if fitness_improvement > 0:\n",
    "            reward = 1.0 + fitness_improvement * 10  # Scale improvement\n",
    "        else:\n",
    "            reward = -0.1  # Small penalty for no improvement\n",
    "        \n",
    "        # Additional reward for maintaining diversity\n",
    "        if diversity_change > 0:\n",
    "            reward += 0.2\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def optimize(self, objective_function, bounds, verbose=True):\n",
    "        \"\"\"Main optimization loop with Q-learning assistance\"\"\"\n",
    "        self.initialize_population(bounds)\n",
    "        \n",
    "        # Evaluate initial population\n",
    "        for i, particle in enumerate(self.particles):\n",
    "            fitness = objective_function(particle)\n",
    "            self.personal_best_fitness[i] = fitness\n",
    "            \n",
    "            if fitness < self.global_best_fitness:\n",
    "                self.global_best_fitness = fitness\n",
    "                self.global_best = particle.copy()\n",
    "        \n",
    "        prev_best_fitness = self.global_best_fitness\n",
    "        prev_diversity = self.calculate_diversity()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            current_diversity = self.calculate_diversity()\n",
    "            improvement_rate = (prev_best_fitness - self.global_best_fitness) / max(abs(prev_best_fitness), 1e-10)\n",
    "            \n",
    "            # Get current state for Q-learning\n",
    "            current_state = self.get_state(iteration, current_diversity, improvement_rate)\n",
    "            \n",
    "            for i in range(self.pop_size):\n",
    "                # Standard PSO velocity and position update\n",
    "                r1, r2 = random.random(), random.random()\n",
    "                \n",
    "                self.velocities[i] = (self.w * self.velocities[i] + \n",
    "                                    self.c1 * r1 * (self.personal_best[i] - self.particles[i]) +\n",
    "                                    self.c2 * r2 * (self.global_best - self.particles[i]))\n",
    "                \n",
    "                self.particles[i] += self.velocities[i]\n",
    "                \n",
    "                # Apply bounds\n",
    "                self.particles[i] = np.clip(self.particles[i], bounds[0], bounds[1])\n",
    "                \n",
    "                # Q-learning assisted local search operation selection\n",
    "                selected_operation = self.select_local_search_operation(current_state)\n",
    "                \n",
    "                # Apply local search operation\n",
    "                old_fitness = objective_function(self.particles[i])\n",
    "                enhanced_particle = self.apply_local_search_operation(self.particles[i], selected_operation)\n",
    "                enhanced_particle = np.clip(enhanced_particle, bounds[0], bounds[1])\n",
    "                \n",
    "                new_fitness = objective_function(enhanced_particle)\n",
    "                \n",
    "                # Accept improvement\n",
    "                if new_fitness < old_fitness:\n",
    "                    self.particles[i] = enhanced_particle\n",
    "                    fitness = new_fitness\n",
    "                else:\n",
    "                    fitness = old_fitness\n",
    "                \n",
    "                # Update personal best\n",
    "                if fitness < self.personal_best_fitness[i]:\n",
    "                    self.personal_best_fitness[i] = fitness\n",
    "                    self.personal_best[i] = self.particles[i].copy()\n",
    "                \n",
    "                # Update global best\n",
    "                if fitness < self.global_best_fitness:\n",
    "                    self.global_best_fitness = fitness\n",
    "                    self.global_best = self.particles[i].copy()\n",
    "                \n",
    "                # Calculate reward and update Q-table\n",
    "                diversity_change = current_diversity - prev_diversity\n",
    "                reward = self.calculate_reward(old_fitness, new_fitness, diversity_change)\n",
    "                \n",
    "                next_diversity = self.calculate_diversity()\n",
    "                next_improvement_rate = (self.global_best_fitness - new_fitness) / max(abs(self.global_best_fitness), 1e-10)\n",
    "                next_state = self.get_state(iteration + 1, next_diversity, next_improvement_rate)\n",
    "                \n",
    "                self.update_q_table(current_state, selected_operation, reward, next_state)\n",
    "            \n",
    "            # Update tracking variables\n",
    "            prev_best_fitness = self.global_best_fitness\n",
    "            prev_diversity = current_diversity\n",
    "            \n",
    "            if verbose and iteration % 20 == 0:\n",
    "                print(f\"Iteration {iteration}: Best fitness = {self.global_best_fitness:.6f}\")\n",
    "        \n",
    "        return self.global_best, self.global_best_fitness\n",
    "\n",
    "# Example usage\n",
    "def sphere_function(x):\n",
    "    \"\"\"Simple sphere function for testing\"\"\"\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def rastrigin_function(x):\n",
    "    \"\"\"Rastrigin function - multimodal test function\"\"\"\n",
    "    A = 10\n",
    "    n = len(x)\n",
    "    return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n",
    "\n",
    "# Test the Q-learning assisted PSO\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize optimizer\n",
    "    ql_pso = QLearningAssistedPSO(dim=10, pop_size=30, max_iter=200)\n",
    "    \n",
    "    # Optimize sphere function\n",
    "    print(\"Optimizing Sphere Function:\")\n",
    "    best_solution, best_fitness = ql_pso.optimize(\n",
    "        sphere_function, \n",
    "        bounds=(-10, 10), \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Best solution: {best_solution}\")\n",
    "    print(f\"Best fitness: {best_fitness}\")\n",
    "    \n",
    "    # Display learned Q-table insights\n",
    "    print(f\"\\nLearned Q-table has {len(ql_pso.q_table)} states\")\n",
    "    for state, actions in list(ql_pso.q_table.items())[:5]:\n",
    "        print(f\"State {state}:\")\n",
    "        for action, q_value in actions.items():\n",
    "            print(f\"  {action}: {q_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
